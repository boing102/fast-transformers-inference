# fast-transformers-inference
Testing out running transformers model inference with 1ms latency
